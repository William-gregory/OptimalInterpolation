{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seaice_freeboard_opt_interp.ipynb","provenance":[{"file_id":"1KFyT-qTCfCeuSJOcHeloi-G8bEFhLYPi","timestamp":1656505856596},{"file_id":"1N555Ryz9fXh9VcusUlnBQeuVN5SKbJ-j","timestamp":1656358218472},{"file_id":"1DkUOseqsfcIN8v4664so26ZRdZ-CIrFl","timestamp":1656078023486},{"file_id":"1SZM0Qraox8Tl_K_emCR-37N06HHcUAYO","timestamp":1655554696866},{"file_id":"16zNmTA28CBYBT52881GZl0gQ5xG3HdmW","timestamp":1655309610540}],"collapsed_sections":[],"authorship_tag":"ABX9TyOncBHsvS0d+pN1aztTqOXK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["install required packages"],"metadata":{"id":"OloXur9Tew-W"}},{"cell_type":"code","source":["import sys\n","sys.version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GF0GW-sOk-2O","executionInfo":{"status":"ok","timestamp":1656512257728,"user_tz":-60,"elapsed":508,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"ee60a4b6-ed80-4d4d-8de3-9cf947a50f5e"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.7.13 (default, Apr 24 2022, 01:04:09) \\n[GCC 7.5.0]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["!pip install gpflow\n","!pip install pyproj\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34DDY7AYW-BL","executionInfo":{"status":"ok","timestamp":1656512263441,"user_tz":-60,"elapsed":5281,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"890ddbe6-2a81-44d4-e00a-07f3aa74dc56"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gpflow in /usr/local/lib/python3.7/dist-packages (2.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gpflow) (1.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gpflow) (21.3)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from gpflow) (0.8.9)\n","Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from gpflow) (1.2.13)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from gpflow) (4.1.1)\n","Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from gpflow) (2.8.2+zzzcolab20220527125636)\n","Requirement already satisfied: tensorflow-probability>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from gpflow) (0.16.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from gpflow) (57.4.0)\n","Requirement already satisfied: lark>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gpflow) (1.1.2)\n","Requirement already satisfied: multipledispatch>=0.6 in /usr/local/lib/python3.7/dist-packages (from gpflow) (0.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpflow) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch>=0.6->gpflow) (1.15.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (1.46.3)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (0.5.3)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (3.17.3)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (0.2.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (2.8.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (3.3.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (0.26.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (1.14.1)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (2.8.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (2.8.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (1.6.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (3.1.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (1.1.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (14.0.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (1.1.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->gpflow) (1.1.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.0->gpflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.4.0->gpflow) (1.5.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (0.6.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (3.3.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.4.0->gpflow) (3.2.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.0->gpflow) (1.3.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.0->gpflow) (4.4.2)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.0->gpflow) (0.1.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gpflow) (3.0.9)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyproj in /usr/local/lib/python3.7/dist-packages (3.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from pyproj) (2022.6.15)\n"]}]},{"cell_type":"markdown","source":["config / parameters\n"],"metadata":{"id":"4HsodxyWW9Qx"}},{"cell_type":"code","source":["branch_name = \"sparse_dev\"\n","# directory on google drive where to \n","# work_sub_dir = [\"MyDrive\", \"workspace\"]"],"metadata":{"id":"m5afwNR9W8q1","executionInfo":{"status":"ok","timestamp":1656512263445,"user_tz":-60,"elapsed":40,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["mount google drive (use to save results)  - requires login"],"metadata":{"id":"uJLFW9rgWK3x"}},{"cell_type":"code","execution_count":20,"metadata":{"id":"wJsq2he6RqVK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656512265251,"user_tz":-60,"elapsed":1839,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"8cb979d4-2919-4404-d7d9-77bb347cf6b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["import subprocess\n","import json\n","from google.colab import drive\n","import os\n","import sys\n","\n","\n","gdrive_mount = '/content/gdrive'\n","# # requires giving access to google drive account\n","drive.mount(gdrive_mount)\n"]},{"cell_type":"markdown","source":["git pull repository"],"metadata":{"id":"owtMH_NNWSqf"}},{"cell_type":"code","source":["import re\n","# change 'workspace' as needed\n","# work_dir = os.path.join(gdrive_mount, *[\"MyDrive\", \"workspace\"])\n","work_dir = \"/content\"\n","\n","# change to working directory\n","# os.chdir(work_dir)\n","assert os.path.exists(work_dir), f\"workspace directory: {work_dir} does not exist\"\n","os.chdir(work_dir)\n","\n","# !git clone https://github.com/William-gregory/OptimalInterpolation.git\n","# url suffix for cloning repp\n","url = \"https://github.com/William-gregory/OptimalInterpolation.git\"\n","\n","# repository directory\n","repo_dir = os.path.join(work_dir, os.path.basename(url))\n","repo_dir = re.sub(\"\\.git$\", \"\", repo_dir)\n","\n","# TODO: put a try except here \n","# clone the repo\n","\n","try:\n","    git_clone = subprocess.check_output( [\"git\", \"clone\", url] , shell=False)\n","except Exception as e:\n","    # get non-zero exit status 128: if the repo already exists?\n","    print(e)\n","\n","print(f\"changing directory to: {repo_dir}\")\n","\n","os.chdir(repo_dir)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhW6ZpIpWN7o","executionInfo":{"status":"ok","timestamp":1656512265253,"user_tz":-60,"elapsed":27,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"e5b34f88-8f47-47e4-bfa3-862f9feeb459"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Command '['git', 'clone', 'https://github.com/William-gregory/OptimalInterpolation.git']' returned non-zero exit status 128.\n","changing directory to: /content/OptimalInterpolation\n"]}]},{"cell_type":"markdown","source":["Change branch "],"metadata":{"id":"8j2KBZ0OZHBM"}},{"cell_type":"code","source":["# --\n","# change branch - review this\n","# --\n","\n","try:\n","    git_checkout = subprocess.check_output([\"git\", \"checkout\", \"-t\", f\"origin/{branch_name}\"], shell=False)\n","    print(git_checkout.decode(\"utf-8\") )\n","except Exception as e:\n","    git_checkout = subprocess.check_output([\"git\", \"checkout\",  f\"{branch_name}\"], shell=False)\n","    print(git_checkout.decode(\"utf-8\") )\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOoGWoAYa83x","executionInfo":{"status":"ok","timestamp":1656512265254,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"753811ea-9804-4a16-ae5e-83100fd92c0f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Your branch is up to date with 'origin/sparse_dev'.\n","\n"]}]},{"cell_type":"code","source":["# git pull to ensure have the latest\n","git_pull = subprocess.check_output([\"git\", \"pull\"], shell=False)\n","print(git_pull.decode(\"utf-8\") )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvJbokw2kKng","executionInfo":{"status":"ok","timestamp":1656512267129,"user_tz":-60,"elapsed":1887,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"808acb2b-c50c-4cf7-9ec4-1402cf42d519"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n","\n"]}]},{"cell_type":"code","source":["# add directory to containing repository to sys.path, so can import as a package\n","# if repo_dir not in sys.path:\n","#     # tmp_dir = os.path.dirname(repo_dir)\n","#     print(f\"adding {repo_dir} to sys.path\")\n","#     sys.path.extend([])\n","\n","if work_dir not in sys.path:\n","    # tmp_dir = os.path.dirname(repo_dir)\n","    print(f\"adding {work_dir} to sys.path\")\n","    sys.path.extend([work_dir])"],"metadata":{"id":"hadf_YaoZXNq","executionInfo":{"status":"ok","timestamp":1656512267135,"user_tz":-60,"elapsed":43,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# TODO: only downlaod if it does not already exist\n","import gdown\n","import zipfile\n","\n","# print(\"there was some sort of issue downloading the entire folder structure\")\n","print(\"will try downloading the zipped version\")\n","# url = \"https://drive.google.com/file/d/1c7h6HTT-wbCq_ZKBYLJSSln4tanlLEMZ\"\n","# id = \"1c7h6HTT-wbCq_ZKBYLJSSln4tanlLEMZ\"\n","\n","\n","data_dir = os.path.join(repo_dir, \"data\")\n","os.makedirs(data_dir, exist_ok=True)\n","\n","id_zip = [\n","    {\"id\": \"1ckoowmCwh4tG76sIxXZuVaSSQ0tv8KTU\", \"zip\": \"auxiliary.zip\"},\n","    {\"id\": \"1cIh9lskzmL6C7EYV8lmJJ5YaJgKqOZHT\", \"zip\": \"CS2S3_CPOM.zip\"},\n","]\n","\n","for _ in id_zip:\n","    id = _['id']\n","    zip = _['zip']\n","    # put data in data dir in repository\n","    output = os.path.join(data_dir, zip)\n","    gdown.download(id=id, output=output, use_cookies=False)\n","\n","    # un zip to path\n","    print(\"unzipping\")\n","    with zipfile.ZipFile(output, 'r') as zip_ref:\n","        zip_ref.extractall(path=data_dir)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a223be83-56a5-4d2e-98f0-8d1b509f8f65","id":"ZJzPX1NuF8cV","executionInfo":{"status":"ok","timestamp":1656512305152,"user_tz":-60,"elapsed":38053,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["will try downloading the zipped version\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1ckoowmCwh4tG76sIxXZuVaSSQ0tv8KTU\n","To: /content/OptimalInterpolation/data/auxiliary.zip\n","100%|██████████| 122M/122M [00:00<00:00, 303MB/s] \n"]},{"output_type":"stream","name":"stdout","text":["unzipping\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1cIh9lskzmL6C7EYV8lmJJ5YaJgKqOZHT\n","To: /content/OptimalInterpolation/data/CS2S3_CPOM.zip\n","100%|██████████| 26.2M/26.2M [00:00<00:00, 221MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["unzipping\n"]}]},{"cell_type":"code","source":["# import os\n","\n","# import os\n","# os.listdir(work_dir)\n","# os.listdir(data_dir)\n","\n","# # REMOVE THIS\n","# print(\"confirming folder structure of content\")\n","# for root, subs, files in os.walk(repo_dir, topdown=True):\n","#     print(\"-\"*50)\n","#     print(f\"root: {root}\")\n","#     print(f\"sub dirs: {subs}\")\n","#     print(f\"files: {files}\")"],"metadata":{"id":"IUXf8f_DGFqM","executionInfo":{"status":"ok","timestamp":1656512305153,"user_tz":-60,"elapsed":18,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# change to repo_dir so can get git info\n","os.chdir(repo_dir)\n","\n","# set output directory\n","output_base_dir = os.path.join(gdrive_mount, \"MyDrive\", \"Dissertation\")"],"metadata":{"id":"_-0IB2pue-_G","executionInfo":{"status":"ok","timestamp":1656512305155,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# calculate the hyper-parameters for GP on freeboard cover using a config\n","# - date(s)\n","# - window size\n","# - radius of inclusion\n","# - freeboard season\n","\n","import warnings\n","import json\n","import os\n","import re\n","import datetime\n","import subprocess\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","from scipy.spatial.distance import squareform, pdist, cdist\n","import scipy.optimize\n","import gpflow\n","from gpflow.utilities import print_summary\n","\n","import time\n","from OptimalInterpolation import get_data_path, get_path\n","from OptimalInterpolation.utils import grid_proj, get_git_information, load_data, split_data2, move_to_archive"],"metadata":{"id":"EUARB7I3i4EZ","executionInfo":{"status":"ok","timestamp":1656512305156,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["\n","# TODO: let config be an input (via sys.argv)\n","# TODO: have an option to calculate using the pure python approach\n","# TODO: allow for prior mean to be determined in a more robust way\n","# TODO: restrict observations to correspond to points where sie cover >= 0.15\n","# TODO: allow for kernel to be specified\n","# TODO: write bad results to file\n","# TODO: allow for over writing, default should be to no allow\n","# TODO: need to be in correct directory to get git info\n","# TODO: consider storing results in json file, one per grid point, can get more info\n","\n","# ---\n","# configuration (defined inline here)\n","# ---\n","\n","config = {\n","    \"dates\": [\"20181201\", \"20190101\", \"20190201\", \"20190301\"], # \"20181201\"\n","    \"inclusion_radius\": 300,\n","    \"days_ahead\": 4,\n","    \"days_behind\": 4,\n","    \"data_dir\": \"package\",\n","    \"season\": \"2018-2019\",\n","    \"grid_res\": 50,\n","    \"coarse_grid_spacing\": 4,\n","    \"min_inputs\": 10,\n","    \"verbose\": 1,\n","    \"initialise_with_neighbors\": False,\n","    # \"hold_out\": [\"S3B\"],\n","    # \"predict_on_hold\": True,\n","    \"scale_inputs\": True,\n","    \"output_dir\": os.path.join(output_base_dir, \"paper_prior_mean_scale_inputs\")\n","}\n","\n","print(\"using config:\")\n","print(json.dumps(config, indent=4))\n","\n","# ----\n","# parameters: extract from config\n","# ----\n","\n","# dates to calculate hyper parameters\n","calc_dates = config['dates']\n","calc_dates = [calc_dates] if not isinstance(calc_dates, list) else calc_dates\n","\n","# radius about a location to include points - in km\n","incl_rad = config.get(\"inclusion_radius\", 300)\n","\n","# directory containing freeboard data, if 'package' is given will use 'data' directory in package\n","datapath = config.get(\"data_dir\", \"package\")\n","if datapath == \"package\":\n","    datapath = get_data_path()\n","\n","# days ahead and behind given date to include for inputs\n","days_ahead = config.get(\"days_ahead\", 4)\n","days_behind = config.get(\"days_behind\", 4)\n","\n","season = config.get(\"season\", \"2018-2019\")\n","# CURRENTLY ONLY ALLOW 2018-2019\n","assert season == \"2018-2019\"\n","\n","grid_res = config.get(\"grid_res\", 25)\n","# min sea ice cover - when loading data set sie to nan if < min_sie\n","min_sie = config.get(\"min_sie\", 0.15)\n","\n","# spacing for coarse grid - let be > 1 to select subset of points\n","coarse_grid_spacing = config.get(\"coarse_grid_spacing\", 1)\n","\n","# min number of inputs to calibrate GP on\n","min_inputs = config.get(\"min_inputs\", 10)\n","\n","# initialise hyper parameters with neighbours values, if they exist\n","init_w_neigh = config.get(\"initialise_with_neighbors\", False)\n","\n","# -\n","# hold out data: used for cross validation\n","hold_out = config.get(\"hold_out\", [])\n","\n","# scale the x,y dimension of the inputs?\n","scale_inputs = config.get(\"scale_inputs\", False)\n","\n","# predict only on hold out locations\n","pred_on_hold_out = config.get(\"predict_on_hold\", False)\n","\n","if len(hold_out):\n","    print(f\"will hold_out data from:\\n{hold_out}\\n(from prediction date)\")\n","    print(f\"pred_on_hold_out (predict only on hold out points) = {pred_on_hold_out}\")\n","\n","output_dir = config[\"output_dir\"]\n","\n","\n","# make an output dir based on parameters\n","# - recall date subdirectories will be added\n","# holdouts = \"\"\n","tmp_dir = f\"radius{incl_rad}_daysahead{days_ahead}_daysbehind{days_behind}_gridres{grid_res}_season{season}_coarsegrid{coarse_grid_spacing}_holdout{'|'.join(hold_out)}\"\n","output_dir = os.path.join(output_dir, tmp_dir)\n","\n","os.makedirs(output_dir, exist_ok=True)\n","# assert os.path.exists(output_dir), f\"output_dir: {output_dir} \\n does not exists, expect it to\"\n","\n","\n","# run time info\n","now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","run_info = {\n","    \"run_datetime\": now\n","}\n","\n","# add git_info\n","try:\n","    run_info['git_info'] = get_git_information()\n","except subprocess.CalledProcessError:\n","    print(\"issue getting git_info, check current working dir\")\n","    pass\n","\n","config[\"run_info\"] = run_info\n","\n","# ---\n","# write config to file\n","# --\n","\n","# TODO: put this somewhere else - to avoid over writing\n","\n","with open(os.path.join(output_dir, \"input_config.json\"), \"w\") as f:\n","    json.dump(config, f, indent=4)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5lfUfn2mI8g","executionInfo":{"status":"ok","timestamp":1656512306084,"user_tz":-60,"elapsed":939,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"6265f79b-24b8-4700-ad20-6979bf21ccb4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["using config:\n","{\n","    \"dates\": [\n","        \"20181201\",\n","        \"20190101\",\n","        \"20190201\",\n","        \"20190301\"\n","    ],\n","    \"inclusion_radius\": 300,\n","    \"days_ahead\": 4,\n","    \"days_behind\": 4,\n","    \"data_dir\": \"package\",\n","    \"season\": \"2018-2019\",\n","    \"grid_res\": 50,\n","    \"coarse_grid_spacing\": 4,\n","    \"min_inputs\": 10,\n","    \"verbose\": 1,\n","    \"initialise_with_neighbors\": false,\n","    \"scale_inputs\": true,\n","    \"output_dir\": \"/content/gdrive/MyDrive/Dissertation/paper_prior_mean_scale_inputs\"\n","}\n"]}]},{"cell_type":"code","source":["t_total0 = time.time()\n","\n","from OptimalInterpolation.data_loader import DataLoader\n","\n","# ----\n","# load data\n","# ----\n","print(\"loading data\")\n","# obs, sie, dates, xFB, yFB, lat, lon = load_data(datapath, grid_res, season,\n","#                                                 dates_to_datetime=False,\n","#                                                 trim_xy=1, min_sie=min_sie)\n","\n","# create a DataLoader object\n","dl = DataLoader(grid_res=f\"{grid_res}km\", seasons=[season], verbose=2)\n","\n","# load aux(iliary) data\n","dl.load_aux_data(aux_data_dir=get_data_path(\"aux\"),\n","                    season=season)\n","# load obs(servation) data\n","dl.load_obs_data(sat_data_dir=get_data_path(\"CS2S3_CPOM\"),\n","                 grid_res=f\"{grid_res}km\")\n","\n","\n","# this contains a (x,y,t) numpy array of only first-year-ice freeboards.\n","# We use this to define the prior mean\n","# cs2_FYI = np.load(\n","#     datapath + '/CS2_25km_FYI_20181101-20190428.npy')\n","\n","# # HARDCODED: drop the first 25 days to align with obs data\n","# # TODO: this should be done in a more systematic way\n","# cs2_FYI = cs2_FYI[..., 25:]\n","\n","# TODO: make sure dates are aligned to satellite data, read in same way\n","cs2_FYI = np.load(\n","    datapath + f'/aux/CS2_{grid_res}km_FYI_20181101-20190428.npy')\n","# create an array of dates\n","cs2_FYI_dates = np.arange(np.datetime64(\"2018-11-01\"), np.datetime64(\"2019-04-29\"))\n","cs2_FYI_dates = np.array([re.sub(\"-\", \"\", i) for i in cs2_FYI_dates.astype(str)])\n","\n","\n","# trimming to align with data\n","xFB = dl.aux['x'][:-1, :-1]\n","yFB = dl.aux['y'][:-1, :-1]\n","lonFB = dl.aux['lon'][:-1, :-1]\n","latFB = dl.aux['lat'][:-1, :-1]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-tz_IrYw0J0u","executionInfo":{"status":"ok","timestamp":1656512306859,"user_tz":-60,"elapsed":811,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}},"outputId":"1effc3b4-ecf3-4e92-e461-65db4ba98198"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["loading data\n","sat_list not provided, using default: ['CS2_SAR', 'CS2_SARIN', 'S3A', 'S3B']\n","loading 'aux' data for season='2018-2019', grid_res='50km'\n","reading 'aux' data\n","reading 'sie' data\n","reading in sat data for: CS2_SAR\n","reading in sat data for: CS2_SARIN\n","reading in sat data for: S3A\n","reading in sat data for: S3B\n","found: 212 common_dates in the data, will use only these dates\n","CS2_SAR: 212\n","CS2_SARIN: 212\n","S3A: 212\n","S3B: 212\n"]}]},{"cell_type":"code","source":["# NOTE: the t values are integers in window, not related to actual dates\n","# x_train, y_train, t_train, z = data_select(date, dates, obs, xFB, yFB,\n","#                                         days_ahead=days_ahead,\n","#                                         days_behind=days_behind)\n","# dl.obs['data'].shape\n","# xFB.shape"],"metadata":{"id":"cLbPuu1Lx8Yz","executionInfo":{"status":"ok","timestamp":1656512306860,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ronald MacEachern","userId":"07664735399186169272"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsqWLSoOiPgB","outputId":"a78a7ad9-8467-4cfb-aeec-a4d9c8b734c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["date_dir: /content/gdrive/MyDrive/Dissertation/paper_prior_mean_scale_inputs/radius300_daysahead4_daysbehind4_gridres50_season2018-2019_coarsegrid4_holdout/20181201\n"," does not exists, creating\n","----------\n","input_config.json not found\n","----------\n","results.csv not found\n","----------\n","prediction.csv not found\n","----------\n","skipped.csv not found\n","selecting data\n","using CS2_FYI data for prior mean - needs review\n","***************************************************************************\n","1/244\n","**********\n","optimization failed! will skip\n","used_n_hp: False, kernel_var: 1.0, noise_var: 1.0\n","grid_loc: (64, 44)\n","***************************************************************************\n","101/244\n","***************************************************************************\n","201/244\n","date_dir: /content/gdrive/MyDrive/Dissertation/paper_prior_mean_scale_inputs/radius300_daysahead4_daysbehind4_gridres50_season2018-2019_coarsegrid4_holdout/20190101\n"," does not exists, creating\n","----------\n","input_config.json not found\n","----------\n","results.csv not found\n","----------\n","prediction.csv not found\n","----------\n","skipped.csv not found\n","selecting data\n","using CS2_FYI data for prior mean - needs review\n","***************************************************************************\n","1/268\n","***************************************************************************\n","101/268\n","***************************************************************************\n","201/268\n","**********\n","optimization failed! will skip\n","used_n_hp: False, kernel_var: 1.0, noise_var: 1.0\n","grid_loc: (148, 84)\n","date_dir: /content/gdrive/MyDrive/Dissertation/paper_prior_mean_scale_inputs/radius300_daysahead4_daysbehind4_gridres50_season2018-2019_coarsegrid4_holdout/20190201\n"," does not exists, creating\n","----------\n","input_config.json not found\n","----------\n","results.csv not found\n","----------\n","prediction.csv not found\n","----------\n","skipped.csv not found\n","selecting data\n","using CS2_FYI data for prior mean - needs review\n","***************************************************************************\n","1/278\n","**********\n","optimization failed! will skip\n","used_n_hp: False, kernel_var: 1.0, noise_var: 1.0\n","grid_loc: (48, 64)\n","**********\n","optimization failed! will skip\n","used_n_hp: False, kernel_var: 1.0, noise_var: 1.0\n","grid_loc: (88, 44)\n","***************************************************************************\n","101/278\n"]}],"source":["\n","\n","# ----\n","# for each date calculate hyper-parameters, make predictions, store values\n","# ----\n","\n","# make a coarse grid - can be used to select a subset of points\n","cgrid = dl.coarse_grid(coarse_grid_spacing,\n","                        grid_space_offset=0,\n","                        x_size=xFB.shape[1],\n","                        y_size=yFB.shape[0])\n","\n","# --\n","# extract data needed for training\n","# --\n","\n","# get the dates from the data\n","dates = dl.obs['dims']['date']\n","# observation data\n","obs = dl.obs['data']\n","# sea ice extent\n","sie = dl.sie['data']\n","\n","# bool array used for projecting onto neighbour\n","# - used because easy to select from sie_day at the same time\n","n_select = np.zeros(cgrid.shape, dtype='bool')\n","\n","\n","# get the datetime of the run\n","now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","for date in calc_dates:\n","\n","    date_dir = os.path.join(output_dir, date)\n","    if not os.path.exists(date_dir):\n","        print(f\"date_dir: {date_dir}\\n does not exists, creating\")\n","        os.makedirs(date_dir)\n","\n","    # ---\n","    # move files to archive, if they already exist\n","    # ---\n","\n","    \n","    move_to_archive(top_dir=date_dir, \n","                    file_names=[\"input_config.json\", \n","                                \"results.csv\",\n","                                \"prediction.csv\",\n","                                \"skipped.csv\"], \n","                    suffix=f\"_{now}\",\n","                    verbose=True)\n","    \n","\n","    # ---\n","    # write config to file - will end up doing this for each date\n","    # ---\n","\n","    with open(os.path.join(date_dir, \"input_config.json\"), \"w\") as f:\n","        json.dump(config, f, indent=4)\n","\n","    # results will be written to file\n","    res_file = os.path.join(date_dir, \"results.csv\")\n","    pred_file = os.path.join(date_dir, \"prediction.csv\")\n","    # bad results will be written to :\n","    skip_file = os.path.join(date_dir, \"skipped.csv\")\n","\n","    # --\n","    # hold out data\n","    # --\n","\n","    # let hold_out_bool flag up where the hold out locations are\n","    # - for the current date\n","    hold_out_bool = np.zeros(dl.obs['data'].shape[:2], dtype=bool)\n","\n","    if len(hold_out):\n","        print(\"some hold out data provided\")\n","        print(hold_out)\n","        # copy observation data\n","        # - so can set hold_out data to np.nan\n","        obs = dl.obs['data'].copy()\n","\n","        for ho in hold_out:\n","            print(f\"removing: {ho} data\")\n","            # get the location of the hold_out (sat)\n","            sat_loc = np.in1d(dl.obs['dims']['sat'], ho)\n","            date_loc = np.in1d(dl.obs['dims']['date'], date)\n","            # get hold_out data observations locations\n","            hold_out_bool[~np.isnan(obs[:, :, sat_loc, date_loc][..., 0])] = True\n","            # set the observations at the hold out location to nan\n","            obs[:, :, sat_loc, date_loc] = np.nan\n","\n","    # select x,y,t and z (freeboard) data for date\n","    # NOTE: the t values are integers in window, not related to actual dates\n","    x_train, y_train, t_train, z = dl.data_select(date, dates, obs, xFB, yFB,\n","                                                    days_ahead=days_ahead,\n","                                                    days_behind=days_behind)\n","\n","    # combine xy data - used for KDtree\n","    xy_train = np.array([x_train, y_train]).T\n","    # make a KD tree for selecting point\n","    X_tree = scipy.spatial.cKDTree(xy_train)\n","\n","    # get the day - which is in the middle of the data, not at start\n","    # - it's location corresponds to date\n","    day = np.where(np.in1d(dates, date))[0][0]\n","\n","    # --\n","    # prior mean\n","    # ---\n","\n","    # mean = np.nanmean(cs2_FYI[..., (day - days_behind):(day + days_ahead + 1)]).round(4)\n","    # TODO: should have checks that range is valid here\n","    print(\"using CS2_FYI data for prior mean - needs review\")\n","    cday = np.where(np.in1d(cs2_FYI_dates, date))[0][0]\n","    # TODO: should this be trailing 31 days?\n","    # mean = np.nanmean(cs2_FYI[..., (cday - days_behind):(cday + days_ahead + 1)]).round(4)\n","    mean = np.nanmean(cs2_FYI[..., (cday - (days_behind + days_ahead + 1)):cday]).round(4)\n","\n","    # ---\n","    # select locations with sea ice cover exists to predict on\n","    # ---\n","\n","    # select bool will determine which locations are predicted for\n","    select_bool = ~np.isnan(sie[..., day]) & cgrid\n","\n","    # if predicting only on the hold out locations, include those in select_bool\n","    if pred_on_hold_out:\n","        print(\"will predict only on hold_out data locations (non nan)\")\n","        # require there are at least some positions to predict on\n","        assert hold_out_bool.any(), f\"pred_on_hold_out: {pred_on_hold_out}\\nhowever hold_out_bool.any(): {hold_out_bool.any()}\"\n","\n","        select_bool = select_bool & hold_out_bool\n","\n","    if not select_bool.any():\n","        warnings.warn(\"there are no points to predict on, will do nothing, check configuration\")\n","\n","    # get the x, y locations\n","    # x_loc, y_loc = xFB[select_bool], yFB[select_bool]\n","    num_loc = select_bool.sum()\n","    select_loc = np.where(select_bool)\n","\n","    # store hyper parameters in a dict for each grid location\n","    # - this is so can initialise hyper parameters with neighbours\n","    # - if  init_w_neigh is True \n","    hp_dict = {}\n","    \n","\n","    # for each location\n","    for i in range(num_loc):\n","\n","        if (i % 100) == 0:\n","            print(\"*\" * 75)\n","            print(f\"{i + 1}/{num_loc + 1}\")\n","\n","        # select locations\n","        grid_loc = select_loc[0][i], select_loc[1][i]\n","        x_ = xFB[grid_loc]\n","        y_ = yFB[grid_loc]\n","\n","        # TODO: move this above\n","        # - this does not work as expect - use the long, lat data from aux\n","        # mplot = grid_proj(llcrnrlon=-90, llcrnrlat=75, urcrnrlon=-152, urcrnrlat=82)\n","        # ln, lt = mplot(x_, y_, inverse=True)\n","\n","        # getting the pre-calculated lon, lat data\n","        ln = lonFB[grid_loc]\n","        lt = latFB[grid_loc]\n","\n","        # get the points from the input data within radius\n","        ID = X_tree.query_ball_point(x=[x_, y_],\n","                                        r=incl_rad * 1000)\n","\n","        if len(ID) < min_inputs:\n","            # print(f\"for (x,y)= ({x_:.2f}, {y_:.2f})\\nthere were only {len(ID)} < {min_inputs} points, skipping\")\n","            tmp = pd.DataFrame({\"grid_loc_0\": grid_loc[0],\n","                                \"grid_loc_1\": grid_loc[1],\n","                                \"reason\": f\"had only {len(ID)} inputs\"},\n","                                index=[i])\n","\n","            tmp.to_csv(skip_file, mode='a',\n","                        header=not os.path.exists(skip_file),\n","                        index=False)\n","            continue\n","\n","        # select points\n","\n","        inputs = np.array([x_train[ID], y_train[ID], t_train[ID]]).T  # training inputs\n","        outputs = z[ID]  # training outputs\n","        n = len(outputs)\n","        mX = np.full(n, mean)\n","\n","        if scale_inputs:\n","            inputs = inputs / np.array([grid_res * 1000, grid_res * 1000, 1.0])\n","\n","        # ----\n","        # GPflow\n","        # ----\n","        t0 = time.time()\n","\n","\n","        # ---\n","        # initialise hyper parameters with neighbours points?\n","        # ---\n","\n","        if scale_inputs:\n","            length_scales = [1.0, 1.0, 1.0]\n","        else:\n","            length_scales = [grid_res * 1000, grid_res * 1000, 1.0]\n","        kernel_var = 1.0\n","        noise_var = 1.0 \n","\n","\n","        used_n_hp = False \n","        if init_w_neigh:\n","            params = []\n","            # TODO: consider different ranges here, allow as input parameter\n","            neigh_range = range(-coarse_grid_spacing, coarse_grid_spacing+1)\n","            for ii in neigh_range:\n","                for jj in neigh_range:          \n","                    # location of neighbour\n","                    n_loc = (grid_loc[0] + ii, grid_loc[1] + jj)\n","                    # if the location already has hyper parameters get those \n","                    # - nested dict\n","                    if n_loc in hp_dict:\n","                        params.append(hp_dict[n_loc])\n","            param_count = len(params)\n","            if param_count:\n","                # print(\"using neighbors hyper parameter values\")\n","                # print(\"params\")\n","                # print(params)\n","                # print(params[0])\n","                ave_param = {hp: sum([p[hp] for p in params]) / param_count\n","                             for hp in [\"ls_y\", \"ls_x\", \"ls_t\", \"kernel_variance\", \"likelihood_variance\"]}\n","                length_scales = [ave_param['ls_x'], ave_param['ls_y'], ave_param['ls_t'] ]\n","                kernel_var = ave_param['kernel_variance']\n","                noise_var = ave_param['likelihood_variance']\n","\n","                # these values can be \n","                if np.isnan(noise_var) | np.isinf(noise_var):\n","                    print(f\"noise_var: {noise_var}\")\n","                    noise_var = 1.0\n","\n","                if np.isnan(kernel_var) | np.isinf(kernel_var):\n","                    print(f\"kernel_var: {kernel_var}\")\n","                    kernel_var = 1.0\n","\n","                used_n_hp = True\n","\n","        # noise_variance can't be too small \n","        noise_var = 1e-5 if noise_var <= 1e-6 else noise_var\n","                                 \n","\n","        # kernel\n","        k = gpflow.kernels.Matern32(lengthscales=length_scales,\n","                                    variance=kernel_var)\n","\n","        # GPR object\n","        m = gpflow.models.GPR(data=(inputs, (outputs - mX)[:, None]),\n","                              kernel=k, mean_function=None,\n","                              noise_variance=noise_var)\n","\n","        # solve for optimal (max log like) hyper parameters\n","        opt = gpflow.optimizers.Scipy()\n","\n","        # %%\n","        opt_logs = opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=10000))\n","        # print_summary(m)\n","\n","        if not opt_logs['success']:\n","            print(\"*\" * 10)\n","            print(\"optimization failed! will skip\")\n","            print(f\"used_n_hp: {used_n_hp}, kernel_var: {kernel_var}, noise_var: {noise_var}\")\n","            print(f\"grid_loc: {grid_loc}\")\n","            tmp = pd.DataFrame({\"grid_loc_0\": grid_loc[0],\n","                    \"grid_loc_1\": grid_loc[1],\n","                    \"reason\": f\"did not converge\"},\n","                    index=[i])\n","\n","            tmp.to_csv(skip_file, mode='a',\n","                        header=not os.path.exists(skip_file),\n","                        index=False)\n","            continue\n","            # length_scales = [grid_res * 1000, grid_res * 1000, 1.0]\n","            # kernel_var = 1.0\n","            # noise_var = 1.0\n","\n","            # # kernel\n","            # k = gpflow.kernels.Matern32(lengthscales=length_scales,\n","            #                             variance=kernel_var)\n","            # # GPR object\n","            # m = gpflow.models.GPR(data=(inputs, (outputs - mX)[:, None]),\n","            #                         kernel=k, mean_function=None,\n","            #                         noise_variance=noise_var)\n","            # # solve for optimal (max log like) hyper parameters\n","            # opt = gpflow.optimizers.Scipy()\n","            # # %%\n","            # opt_logs = opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=10000))\n","\n","            # print(f\"opt_logs['success']: {opt_logs['success']}\")\n","            # print(f\"grid_loc: {grid_loc}\")\n","\n","\n","        t1 = time.time()\n","\n","        # get the run time\n","        run_time = t1 - t0\n","\n","        # extract the hyper parameters\n","        gpf_hyp = np.concatenate([m.kernel.lengthscales.numpy(),\n","                                    np.atleast_1d(m.kernel.variance.numpy()),\n","                                    np.atleast_1d(m.likelihood.variance.numpy())])\n","\n","        # TODO: double check to see if these are the right way around \n","        #  - this the first dimension not y?\n","        lscale = {nn: m.kernel.lengthscales.numpy()[_]\n","                    for _, nn in enumerate([\"ls_x\", \"ls_y\", \"ls_t\"])}\n","\n","        if scale_inputs:\n","            lscale['ls_x'] *= (grid_res * 1000)\n","            lscale['ls_y'] *= (grid_res * 1000)\n","\n","        res = {\n","            \"date\": date,\n","            \"x\": x_,\n","            \"y\": y_,\n","            \"lon\": ln,\n","            \"lat\": lt,\n","            \"grid_loc_0\": grid_loc[0],\n","            \"grid_loc_1\": grid_loc[1],\n","            \"num_inputs\": len(ID),\n","            \"used_n_hp\": used_n_hp,\n","            # \"lengthscales\": m.kernel.lengthscales.numpy(),\n","            **lscale,\n","            \"kernel_variance\": float(m.kernel.variance.numpy()),\n","            \"likelihood_variance\": float(m.likelihood.variance.numpy()),\n","            \"run_time\": run_time,\n","            \"loglike\": m.log_marginal_likelihood().numpy(),\n","            \"mean\": mean\n","        }\n","\n","        tmp = pd.DataFrame(res, index=[i])\n","\n","        # append results to file\n","        tmp.to_csv(res_file, mode=\"a\", header=not os.path.exists(res_file),\n","                    index=False)\n","        \n","        # store hyper parameters in dict - using grid_loc as key\n","        if init_w_neigh:\n","            hp_dict[( grid_loc[0],  grid_loc[1])] = {i: res[i] \n","                                                     for i in ['ls_x', 'ls_y', 'ls_t', 'kernel_variance', 'likelihood_variance']}\n","\n","        # ---\n","        # project to points near by, based on grid spacing\n","        # ---\n","\n","        # extract parameters / projection\n","        # - select neighbouring points\n","        # TODO: see if there is a neater way of doing this,\n","\n","        t0 = time.time()\n","\n","        g0, g1 = grid_loc\n","        l0 = np.max([0, g0 - coarse_grid_spacing])\n","        u0 = np.min([n_select.shape[0], g0 + coarse_grid_spacing + 1])\n","        l1 = np.max([0, g1 - coarse_grid_spacing])\n","        u1 = np.min([n_select.shape[1], g1 + coarse_grid_spacing + 1])\n","\n","        # select neighbouring points\n","        n_select[l0:u0, l1:u1] = True\n","        n_bool = ~np.isnan(sie[..., day]) & n_select\n","        # set points back\n","        n_select[l0:u0, l1:u1] = False\n","        n_select_loc = np.where(n_bool)\n","\n","        x_s = xFB[n_select_loc][:, None]\n","        y_s = yFB[n_select_loc][:, None]\n","        lon_s = lonFB[n_select_loc][:, None]\n","        lat_s = latFB[n_select_loc][:, None]\n","        t_s = np.ones(len(x_s))[:, None] * days_behind\n","        xs = np.concatenate([x_s, y_s, t_s], axis=1)\n","\n","        y_pred = m.predict_y(Xnew=xs)\n","        f_pred = m.predict_f(Xnew=xs)\n","\n","        t1 = time.time()\n","\n","        pred = {\n","            \"grid_loc_0\": grid_loc[0],\n","            \"grid_loc_1\": grid_loc[1],\n","            \"proj_loc_0\": n_select_loc[0],\n","            \"proj_loc_1\": n_select_loc[1],\n","            \"x\": x_s[:, 0],\n","            \"y\": y_s[:, 0],\n","            \"lon\": lon_s[:, 0],\n","            \"lat\": lat_s[:, 0],\n","            \"f*\": f_pred[0].numpy()[:, 0],\n","            \"f*_var\": f_pred[1].numpy()[:, 0],\n","            \"y_var\": y_pred[1].numpy()[:, 0],\n","            \"mean\": mean,\n","            \"run_time\": t1 - t0\n","        }\n","\n","        tmp = pd.DataFrame(pred)\n","\n","        # append results to file\n","        tmp.to_csv(pred_file, mode=\"a\", header=not os.path.exists(pred_file),\n","                    index=False)\n","        \n"]},{"cell_type":"code","source":["t_total1 = time.time()\n","print(f\"total run time: {t_total1 - t_total0:.2f}\")\n","\n","with open(os.path.join(output_dir, \"total_runtime.txt\"), \"+w\") as f:\n","    f.write(f\"runtime: {t_total1 - t_total0:.2f} seconds\")"],"metadata":{"id":"DV3TB3GQmP1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noise_var"],"metadata":{"id":"mNWZL7DKrycQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # from will\n","# import cartopy.crs as ccrs\n","# import cartopy.feature as cfeat\n","\n","# fig, ax = plt.subplots(1, figsize=(5, 5),\n","#                        subplot_kw=dict(projection=ccrs.NorthPolarStereo()))\n","# ax.coastlines(resolution='50m', color='white')\n","# ax.add_feature(cfeat.LAKES, color='white', alpha=.5)\n","# # ax.add_feature(cfeat.RIVERS, color='white', alpha=.1)\n","# ax.add_feature(cfeat.LAND, color=(0.8, 0.8, 0.8))\n","\n","# ax.set_extent([-180, 180, 60, 90], ccrs.PlateCarree())  # lon_min,lon_max,lat_min,lat_max\n","\n","# plt.show()"],"metadata":{"id":"pcIbc5A7mcOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ncm8uj8jnWCY"},"execution_count":null,"outputs":[]}]}